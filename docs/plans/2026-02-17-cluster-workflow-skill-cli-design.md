# Cluster Workflow: Skill + CLI Design

Date: 2026-02-17
Status: Approved
Replaces: `docs/plans/2026-02-12-mcp-run-manager-v01-design.md`

## 1. Background

The HeisenbergVMCPEPS project runs finite-size quantum spin simulations on HPC
clusters (currently `susphy`). The workflow is a multi-stage pipeline:

1. Simple update (imaginary time evolution on unit cell)
2. Tile unit-cell TPS to full lattice
3. VMC optimization (MPI)
4. MC measurement (MPI)

Previously, this was managed via hand-crafted monolithic slurm scripts (~150
lines with embedded bash convergence logic). An MCP server (`mcp/run-manager/`)
was designed but never deployed. This design replaces both approaches with a
**skill-first architecture** backed by a lightweight Python CLI.

## 2. Decision: Delete Unused Infrastructure

The following untracked files are deleted:

- `mcp/run-manager/` -- Python MCP server, never configured or used
- `docs/plans/2026-02-12-mcp-run-manager-v01-design.md` -- superseded by this doc
- `skills/SKILL.md` -- old run-manager skill, conventions don't match reality

Preserved and incorporated:

- `skills/susphy_cluster_run_notes.md` -- cluster operational knowledge folded
  into the new skill

## 3. Architecture

```
User <-> Claude Code <-> Skill (workflow knowledge)
                     <-> CLI (scripts/cluster_run.py)
                     <-> SSH via Bash (cluster interaction)
```

- **Skill** encodes workflow conventions, parameter defaults, checklists
- **CLI** generates case directories, param files, slurm scripts from templates
- **SSH** is used directly (no MCP server, no Python intermediary)

## 4. Composable Stages

The code has built-in `TauSchedule` and `AdvancedStop` features, eliminating
the need for bash convergence loops. Stages map to slurm jobs:

| Job | Stages | Binary | Resources |
|-----|--------|--------|-----------|
| Job 1 | Simple update + tile | `simple_update`, `sitps_tile` | Single node |
| Job 2 | VMC optimize | `vmc_optimize` (MPI) | Multi-rank, `--dependency=afterok:JOB1` |
| Job 3 | MC measure | `mc_measure` (MPI) | Multi-rank, `--dependency=afterok:JOB2` |

Tiling is lightweight (seconds) and runs as part of Job 1 rather than a
separate submission.

## 5. Directory Convention

```
run/
  <case>/                              e.g. 8x8J2=0D8
    <timestamp>/                       e.g. 20260217_211901
      params/
        physics_unit.json              unit cell (e.g. 2x2 PBC)
        physics_full.json              full lattice (e.g. 8x8 PBC)
        simple_update.json             tau-schedule + advanced-stop
        vmc.json                       optimizer config
        measure.json                   MC measurement config
      stage1_su_tile.slurm
      stage2_vmc.slurm
      stage3_measure.slurm
      su_unit/                         simple update working dir
        tau_schedule/                  auto-generated by binary
        tpsfinal/
        peps/
      tpsfinal_tiled/                  tiled full-size TPS
      vmc/                             VMC working dir
        energy/
        tpsfinal/
        checkpoints/
      measure/                         measurement working dir
        stats/
        tpsfinal/
      logs/                            slurm stdout/stderr
      meta.json                        run metadata
```

Case name format: `<Lx>x<Ly>J2=<J2>D<D>` (e.g. `8x8J2=0D8`).

## 6. CLI: `scripts/cluster_run.py`

### 6.1 Commands

**`new-case`** -- Generate a complete case directory locally.

```bash
python3 scripts/cluster_run.py new-case \
  --lattice 8x8 --D 8 --J2 0.0 --bc pbc \
  --unit-cell 2x2 \
  --cluster susphy --partition 256G56c --ntasks 56
```

Optional overrides:
- `--optimizer SR|Adam|AdaGrad|SGD` (default: SR)
- `--lr 0.1` (default: 0.1)
- `--iterations 30` (default: 30)
- `--taus "0.5,0.2,0.1,0.05,0.02"` (default: standard sequence)
- `--override key=value` (expert knobs)

Output: prints parameter summary for review, generates files under
`run/<case>/<timestamp>/`.

**`submit`** -- Upload and submit all stages with dependency chaining.

```bash
python3 scripts/cluster_run.py submit \
  --case run/8x8J2=0D8/20260217_211901 \
  --cluster susphy
```

Submits stage1, then stage2 with `--dependency=afterok:JOB1`, then stage3 with
`--dependency=afterok:JOB2`. Records job IDs in `meta.json`.

**`status`** -- Check running jobs on cluster.

```bash
python3 scripts/cluster_run.py status --cluster susphy
```

**`fetch`** -- Pull results (energy trajectory + measurement stats) to local.

```bash
python3 scripts/cluster_run.py fetch \
  --case run/8x8J2=0D8/20260217_211901 \
  --cluster susphy --output-dir data/
```

### 6.2 Parameter Tiers

| Tier | Parameters | How set |
|------|-----------|---------|
| Must-specify | `--lattice`, `--D`, `--J2`, `--bc` | CLI flags, error if missing |
| Smart defaults | TRGDmin, MC samples, tau schedule, advanced-stop, SR config | Auto-computed, shown in summary |
| Expert knobs | CGMaxIter, spike thresholds, checkpoint interval | `--override key=value` |

### 6.3 MC Sample Scaling

VMC optimization:
- `MC_total_samples = max(10000, min(20000, Lx * Ly * 200))`
- Rounded to nearest multiple of `ntasks`

MC measurement:
- `MC_total_samples = 2 * VMC_samples`

### 6.4 TRG Dimension Defaults

- `TRGDmin = D` (matches PEPS bond dimension)
- `TRGDmax = 2 * D` (reasonable contraction accuracy)

### 6.5 Simple Update Defaults

Uses built-in `TauSchedule` + `AdvancedStop`:

```json
{
  "TauScheduleEnabled": true,
  "TauScheduleTaus": "0.5,0.2,0.1,0.05,0.02",
  "TauScheduleRequireConverged": true,
  "TauScheduleDumpEachStage": true,
  "TauScheduleDumpDir": "tau_schedule",
  "AdvancedStopEnabled": true,
  "AdvancedStopEnergyAbsTol": 1e-6,
  "AdvancedStopEnergyRelTol": 1e-8,
  "AdvancedStopLambdaRelTol": 1e-5,
  "AdvancedStopPatience": 3,
  "AdvancedStopMinSteps": 10
}
```

## 7. Skill: `vmcpeps-cluster-workflow`

Single skill file replacing `SKILL.md` and `susphy_cluster_run_notes.md`.

Contents:
1. Workflow overview (3-stage pipeline)
2. CLI command reference
3. Parameter conventions and defaults
4. Cluster operational notes (partition selection, env loading, failure
   signatures)
5. Result interpretation (energy trajectory format, measurement stats)
6. Pre-submit checklist (show summary, confirm with user, verify SSH)

## 8. Slurm Script Templates

### 8.1 Stage 1: Simple Update + Tile

~30 lines. Single-node job. Runs `simple_update` with tau-schedule params,
then `sitps_tile` to expand unit cell to full lattice. Copies tiled TPS into
`vmc/tpsfinal/` for stage 2.

### 8.2 Stage 2: VMC Optimize

~20 lines. MPI job. Runs `vmc_optimize` in `vmc/` directory.

### 8.3 Stage 3: MC Measure

~20 lines. MPI job. Runs `mc_measure` in `measure/` directory. Copies
`vmc/tpsfinal/` to `measure/tpsfinal/` before running.

## 9. Implementation Scope

| Component | Estimated size | Priority |
|-----------|---------------|----------|
| `scripts/cluster_run.py` | ~400 lines | P0 |
| Skill file | ~150 lines | P0 |
| Delete old files | -- | P0 |
| Slurm templates (embedded in CLI) | ~70 lines total | P0 |

Runtime: Python stdlib + SSH only. Dev dependency: pytest.

## 10. What This Design Does NOT Cover

- Multi-cluster support (only `susphy` for now)
- Automated plotting
- Parameter sweeps (generate multiple cases)
- Job cancellation CLI command
- Resuming VMC from checkpoints
- Historical data migration

These can be added incrementally as needed.
